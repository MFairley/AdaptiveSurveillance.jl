        - using Optim, NLSolversBase, Random, Distributions
        - using StatsBase
        - import Convex, Mosek, MosekTools
        - using Plots
        - 
        - ### Optim
        - function normalized_log_likelihood(β::Float64, z::Float64, Γ::Int64, t::Array{Int64}, W::Array{Float64}, n::Int64)
    50096     tΓ = max.(0, t .- Γ)
    50096     p = logistic.(β * tΓ .+ z)
        0     return sum(logpdf(Binomial(n, p[i]), W[i]) for i = 1:length(W))
        - end
        - 
        - function log_likelihood(x, tΓ::Array{Int64}, W::Array{Float64}, n::Int64)
        -     β, z = x[1], x[2]
        -     coeff = β .* tΓ .+ z
        -     return -sum(W .* coeff .- n .* log1pexp.(coeff))
        - end
        - 
        - function log_likelihood_grad!(g::Array{Float64}, x::Array{Float64}, tΓ::Array{Int64}, W::Array{Float64}, n::Int64)
        -     β, z = x[1], x[2]
        -     coeff = β .* tΓ .+ z
        -     sigd1 = logistic.(coeff)
        -     g[1] = -sum(W .* tΓ .- n .* sigd1 .* tΓ)
        -     g[2] = -sum(W .- n .* sigd1)
        - end
        - 
        - function log_likelihood_hess!(h::Array{Float64}, x::Array{Float64}, tΓ::Array{Int64}, n::Int64)
        -     β, z = x[1], x[2]
        -     coeff = β .* tΓ .+ z
        -     sigd2 = logistic.(coeff) .* (1 .- logistic.(coeff))
        -     h[1, 1] = -sum(-n .* (sigd2 .* tΓ.^2))
        -     h[1, 2] = -sum(-n .* tΓ .* sigd2)
        -     h[2, 1] = h[1, 2]
        -     h[2, 2] = -sum(-n .* sigd2)
        - end
        - 
        - function log_likelihood_fgh!(f::Union{Float64, Nothing}, g::Union{Array{Float64}, Nothing}, h::Union{Array{Float64}, Nothing}, x::Array{Float64}, tΓ::Array{Int64}, W::Array{Float64}, n::Int64)
        0     β, z = x[1], x[2]
286988576     coeff = β .* tΓ .+ z
        0     if !isnothing(g)
205754688         sigd1 = logistic.(coeff)
205754688         g[1] = -sum(W .* tΓ .- n .* sigd1 .* tΓ)
205754688         g[2] = -sum(W .- n .* sigd1)
        -     end
        0     if !isnothing(h)
 81233888         sigd2 = logistic.(coeff) .* (1 .- logistic.(coeff))
 81233888         h[1, 1] = -sum(-n .* (sigd2 .* tΓ.^2))
 81233888         h[1, 2] = -sum(-n .* tΓ .* sigd2)
        0         h[2, 1] = h[1, 2]
 81233888         h[2, 2] = -sum(-n .* sigd2)
        -     end
        -     if !isnothing(f)
286988576         return -sum(W .* coeff .- n .* log1pexp.(coeff))
        -     end
        -     nothing
        -   end
        - 
        - function solve_logistic_Γ_subproblem_optim(Γ::Int64, t::Array{Int64}, W::Array{Float64}, n::Int64, x0 = [0.01, logit(0.01)], ux = [1.0, logit(0.5)])
  3891328     tΓ = max.(0, t .- Γ)
        -     # fun = (x) -> log_likelihood(x, tΓ, W, n)
        -     # fun_grad! = (g, x) -> log_likelihood_grad!(g, x, tΓ, W, n)
        -     # fun_hess! = (h, x) -> log_likelihood_hess!(h, x, tΓ, n)
        0     fun_fgh! = (f, g, h, x) -> log_likelihood_fgh!(f, g, h, x, tΓ, W, n)
        -     
        -     # df = TwiceDifferentiable(fun, fun_grad!, fun_hess!, x0)
        0     df = TwiceDifferentiable(Optim.only_fgh!(fun_fgh!), x0)
  1990912     dfc = TwiceDifferentiableConstraints([0.0, -Inf], ux)
        0     res = optimize(df, dfc, x0, IPNewton())
    90496     obj = -Optim.minimum(res)
        0     β, z = Optim.minimizer(res)
        - 
        0     return obj, β, z
        - end
        - 
        - function solve_logistic_optim(t::Array{Int64}, W::Array{Float64}, n::Int64)
        -     max_obj, βs, zs, Γs = -Inf, 0.0, 0.0, 0
        0     for Γ = 0:maximum(t) # Threads.@threads # to do, fix type instability here 
        0         obj, β, z = solve_logistic_Γ_subproblem_optim(Γ, t, W, n)
        -         # obj, β, z = solve_logistic_Γ_subproblem_convex(Γ, t, W, n)
        0         if obj >= max_obj
        -             max_obj = obj
        0             βs, zs, Γs = β, z, Γ
        -         end
        -     end
        0     return max_obj, βs, zs, Γs
        - end
        - 
        - function profile_log_likelihood(n1::Int64, n2::Int64, tp::Int64, t::Array{Int64}, W::Array{Float64}, n::Int64)
        0     @assert n1 <= n2
        0     @assert tp > maximum(t)
      512     W = vcat(W, n1)
      512     t = vcat(t, tp)
      896     lp = zeros(n2 - n1 + 1)
        0     for (j, i) in enumerate(n1:n2)
        0         W[end] = i # this makes this not parallel
        0         _, β, z, Γ = solve_logistic_optim(t, W, n)
        0         lp[j] = normalized_log_likelihood(β, z, Γ, t, W, n)
        -     end
        0     return lp
        - end
        - 
        - function future_alarm_log_probability(n1, n2, tp, W, t, n)
        0     return logsumexp(profile_log_likelihood(n1, n2, tp, t, W, n))
        - end
        - 
        - function profile_likelihood(tp, t, W, n)
        -     return softmax(profile_log_likelihood(0, n, tp, t, W, n))
        - end
        - 
        - function plot_profile_likelihood(tp, t, W, n; path = "")
        -     pl = profile_likelihood(tp, t, W, n)
        -     bar(n1:n2, pl, xlabel = "Number of Positive Tests", ylabel = "Probability", 
        -         legend=false, title = "Profile Likelihood for time $(tp) at time $(Int(maximum(t)))")
        -     savefig(joinpath(path, "profile_likelihood_$(tp)_$(Int(maximum(t))).pdf"))
        -     return pl
        - end
        - 
        - ### Convex.jl
        - function solve_logistic_Γ_subproblem_convex(Γ, t, W, n, ux = [1.0, logit(0.5)])
        -     tΓ = max.(0, t .- Γ)
        -     β = Convex.Variable(Convex.Positive())
        -     z = Convex.Variable()
        -     coeff = β * tΓ + z
        -     obj = Convex.dot(W, coeff) - n * Convex.logisticloss(coeff)
        -     problem = Convex.maximize(obj, β <= ux[1], z <= ux[2])
        -     Convex.solve!(problem, () -> Mosek.Optimizer(QUIET=true), verbose=false)
        -     return problem.optval, Convex.evaluate(β), Convex.evaluate(z)
        - end
